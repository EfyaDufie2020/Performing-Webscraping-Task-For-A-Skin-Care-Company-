{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0da0735",
   "metadata": {},
   "source": [
    "### Performing Web Scraping For A Skin Care Company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeb70c",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Veefyed (client) wants to understand the product catalog from AfroGlamour Cosmetics. Right now, the data is unstructured, scattered across web pages, and not available in a format that supports analytics or reporting.\n",
    "\n",
    "By scraping, cleaning, and structuring this data, Veefyed can:\n",
    "\n",
    "1. Analyze product features and pricing.\n",
    "\n",
    "2. Identify common ingredients and their frequency.\n",
    "\n",
    "3. Better understand which products align with customer skin concerns.\n",
    "\n",
    "4. Store all images for marketing and catalog purposes.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The product information on AfroGlamour Cosmetics’ website is not centralized. It exists only as HTML content and scattered images.\n",
    "\n",
    "- No single file contains all product details.\n",
    "- Images are not downloadable in bulk.\n",
    "\n",
    "- Ingredient lists are long and inconsistent.\n",
    "\n",
    "This makes it difficult for analysts and business teams to:\n",
    "\n",
    "- Compare products side by side.\n",
    "\n",
    "- Create structured reports.\n",
    "\n",
    "- Identify commonalities (like ingredients for skin concerns).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this project was to collect structured product data from AfroGlamour Cosmetics and organize it into a clean dataset. This included extracting key product details and ensuring that all images associated with each product were downloaded in good quality.\n",
    "\n",
    "\n",
    "### Stakeholders\n",
    "\n",
    "- Business/Marketing Team – Needs product data for promotions, price comparisons, and catalog building.\n",
    "\n",
    "- Data Analysts – Require structured datasets to perform ingredient analysis, trend analysis, and pricing insights.\n",
    "\n",
    "- Product Development Team – Can analyze ingredients and skin concerns to identify product gaps.\n",
    "\n",
    "- Customers (Indirect Stakeholders) – Benefit when product data is clear, consistent, and easier to search or recommend.\n",
    "\n",
    "- Tech/Data Team – Responsible for collecting, cleaning, and structuring the raw product data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Key Features of the Solution\n",
    "\n",
    "1. Web Scraping Engine\n",
    "Collects product details (ID, brand, description, price, etc.) from product pages.\n",
    "\n",
    "2. Image Extraction & Download\n",
    "Downloads all product images in high quality, stores them in a local folder, and links them in the dataset.\n",
    "\n",
    "3. Data Structuring (CSV/Excel)\n",
    "Clean, tabular dataset for easy use in Excel, Power BI, or Python notebooks.\n",
    "\n",
    "4. Ingredient Grouping Table\n",
    "Frequency count of ingredients across products to highlight most used ingredients.\n",
    "\n",
    "5. Scalable Approach\n",
    "Code can be extended to scrape more products, or even other e-commerce sites, with minimal changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed02d78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a36d2efd",
   "metadata": {},
   "source": [
    "1. Importing all the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e392c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import shutil\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c29eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config - adjust as needed\n",
    "BASE = \"https://afroglamourcosmetics.com\"\n",
    "SHOP_PAGE = urljoin(BASE, \"/shop/\")\n",
    "OUTPUT_CSV = \"products.csv\"\n",
    "OUTPUT_XLSX = \"products.xlsx\"\n",
    "IMAGES_DIR = \"images\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/100 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Behavior flags\n",
    "USE_SELENIUM = False        # set True if pages are JS-driven (not used by default)\n",
    "DOWNLOAD_IMAGES = True      # set False to skip downloading images (faster)\n",
    "PARALLEL_DOWNLOADS = True   # use threads for image downloads\n",
    "N_PRODUCTS = 12             # number of products to scrape\n",
    "SAVE_INTERVAL = 5           # save partial CSV every N products\n",
    "MAX_WORKERS = 6             # threads for image downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1258666",
   "metadata": {},
   "source": [
    "### Session and robust request helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a65ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Session + safe_request\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def safe_request(url, session=session, retries=3, backoff=1.0, timeout=20):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"[safe_request] Failed: {url} -> {e}\")\n",
    "                return None\n",
    "            time.sleep(backoff * (attempt + 1))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42a4c8",
   "metadata": {},
   "source": [
    "Utility helpers (product id, partial save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf73960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Helpers\n",
    "def make_product_id(url):\n",
    "    path = urlparse(url).path.rstrip(\"/\")\n",
    "    last = path.split(\"/\")[-1] or str(uuid.uuid4())[:8]\n",
    "    return re.sub(r'[^A-Za-z0-9_\\-]', '_', last)\n",
    "\n",
    "def save_partial(rows, csv_path=\"products_partial.csv\"):\n",
    "    try:\n",
    "        pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "        print(f\"[save_partial] saved {csv_path} ({len(rows)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(\"[save_partial] failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3403968",
   "metadata": {},
   "source": [
    "Robust image downloader (streaming + size check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c183935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Robust image downloader\n",
    "\n",
    "def download_image(url, save_folder, session=session, retries=4, timeout=25):\n",
    "    if not url:\n",
    "        return None\n",
    "    if isinstance(url, str) and \",\" in url and \" \" in url:\n",
    "        url = url.split(\",\")[0].strip().split(\" \")[0]\n",
    "    url = urljoin(BASE, url)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = session.get(url, stream=True, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            path = urlparse(url).path\n",
    "            ext = os.path.splitext(path)[1].split('?')[0] or \".jpg\"\n",
    "            fname = re.sub(r'[^A-Za-z0-9_.-]', '_', os.path.basename(path)) or f\"{uuid.uuid4().hex[:8]}{ext}\"\n",
    "            filepath = os.path.join(save_folder, fname)\n",
    "            with open(filepath, \"wb\") as out:\n",
    "                shutil.copyfileobj(resp.raw, out)\n",
    "            if os.path.getsize(filepath) < 1024:\n",
    "                os.remove(filepath)\n",
    "                raise ValueError(\"Downloaded file too small\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"[download_image] failed {url} -> {e}\")\n",
    "                return None\n",
    "            time.sleep(0.5 * (attempt + 1))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40795629",
   "metadata": {},
   "source": [
    "Parse product page (robust selectors + many image attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98beba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. parse_product_page\n",
    "\n",
    "def parse_product_page(url):\n",
    "    r = safe_request(url)\n",
    "    if not r:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # product name\n",
    "    name_tag = soup.find([\"h1\", \"h2\"], attrs={\"class\": re.compile(r\".*product.*\", re.I)}) or soup.find(\"h1\")\n",
    "    product_name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "    # price\n",
    "    price = None\n",
    "    price_tag = soup.find(string=re.compile(r\"AED|USD|£|\\$\"))\n",
    "    if price_tag:\n",
    "        p = re.search(r\"([A-Z]{2,3}\\s*)?(\\d{1,3}(?:[.,]\\d{2,})?)\", price_tag)\n",
    "        if p:\n",
    "            price = p.group(0).strip()\n",
    "\n",
    "    # sku / barcode\n",
    "    sku = None\n",
    "    m = re.search(r\"SKU[:\\s]*([A-Za-z0-9\\-\\_]+)\", page_text, re.I)\n",
    "    if m:\n",
    "        sku = m.group(1).strip()\n",
    "    else:\n",
    "        m2 = re.search(r\"\\b(\\d{12,13})\\b\", page_text)\n",
    "        if m2:\n",
    "            sku = m2.group(1)\n",
    "\n",
    "    # description\n",
    "    desc = \"\"\n",
    "    desc_tag = soup.find(id=\"tab-description\") or soup.find(\"div\", class_=re.compile(r\"description\", re.I))\n",
    "    if desc_tag:\n",
    "        desc = desc_tag.get_text(\" \", strip=True)\n",
    "    else:\n",
    "        p = soup.find(\"p\")\n",
    "        desc = p.get_text(\" \", strip=True) if p else \"\"\n",
    "\n",
    "    # brand / product line\n",
    "    brand = None\n",
    "    b_tag = soup.find(string=re.compile(r\"Brand|brand|Category:\", re.I))\n",
    "    if b_tag:\n",
    "        try:\n",
    "            brand = b_tag.find_next().get_text(strip=True)\n",
    "        except Exception:\n",
    "            brand = None\n",
    "    if not brand:\n",
    "        bc = soup.find(\"nav\", class_=re.compile(r\"breadcrumb\", re.I))\n",
    "        if bc:\n",
    "            items = [it.get_text(strip=True) for it in bc.find_all(\"a\")]\n",
    "            if items:\n",
    "                brand = items[-1]\n",
    "\n",
    "    # ingredients\n",
    "    ingredients = None\n",
    "    ing_tag = soup.find(string=re.compile(r\"Ingredients\", re.I))\n",
    "    if ing_tag:\n",
    "        parent = ing_tag.parent\n",
    "        nxt = parent.find_next([\"p\", \"ul\", \"div\"])\n",
    "        if nxt:\n",
    "            ingredients = nxt.get_text(\" \", strip=True)\n",
    "\n",
    "    # size/volume\n",
    "    size_volume = None\n",
    "    msize = re.search(r\"(\\d+\\s?(?:ml|mL|g|gm|oz|litre|l))\", page_text, re.I)\n",
    "    if msize:\n",
    "        size_volume = msize.group(1)\n",
    "\n",
    "    # skin concern inference\n",
    "    lower = (desc or \"\").lower() + \" \" + page_text.lower()\n",
    "    skin_concern = None\n",
    "    if any(word in lower for word in [\"brighten\", \"lighten\", \"whiten\", \"dark spot\"]):\n",
    "        skin_concern = \"brightening/dark-spots\"\n",
    "    elif any(word in lower for word in [\"moistur\", \"hydr\", \"dry\"]):\n",
    "        skin_concern = \"moisturizing/dry-skin\"\n",
    "    elif \"anti-aging\" in lower or \"age\" in lower:\n",
    "        skin_concern = \"anti-aging\"\n",
    "    elif \"acne\" in lower or \"pimple\" in lower:\n",
    "        skin_concern = \"acne\"\n",
    "\n",
    "    # images\n",
    "    imgs = []\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = (\n",
    "            img.get(\"src\")\n",
    "            or img.get(\"data-src\")\n",
    "            or img.get(\"data-lazy-src\")\n",
    "            or img.get(\"data-srcset\")\n",
    "            or img.get(\"srcset\")\n",
    "        )\n",
    "        if not src:\n",
    "            continue\n",
    "        if isinstance(src, str) and \",\" in src:\n",
    "            src = src.split(\",\")[0].strip().split(\" \")[0]\n",
    "        src = urljoin(BASE, src)\n",
    "        if any(x in src for x in [\"logo\", \"sprite\", \"placeholder\", \"icon\"]):\n",
    "            continue\n",
    "        imgs.append(src)\n",
    "    imgs = list(dict.fromkeys(imgs))\n",
    "\n",
    "    return {\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"sku\": sku,\n",
    "        \"description\": desc,\n",
    "        \"brand\": brand,\n",
    "        \"ingredients\": ingredients,\n",
    "        \"size_volume\": size_volume,\n",
    "        \"skin_concern\": skin_concern,\n",
    "        \"image_urls\": imgs,\n",
    "        \"source_url\": url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0923d17",
   "metadata": {},
   "source": [
    "Get product links from shop pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fa1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. get_product_links_from_shop_pages\n",
    "\n",
    "def get_product_links_from_shop_pages(required=N_PRODUCTS, start_page=1):\n",
    "    links = []\n",
    "    page = start_page\n",
    "    while len(links) < required:\n",
    "        shop_url = f\"{BASE}/shop/page/{page}/\" if page > 1 else SHOP_PAGE\n",
    "        r = safe_request(shop_url)\n",
    "        if not r:\n",
    "            print(f\"[get_product_links] failed to load {shop_url}\")\n",
    "            break\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if \"/product/\" in href:\n",
    "                full = urljoin(BASE, href)\n",
    "                if full not in links:\n",
    "                    links.append(full)\n",
    "        page += 1\n",
    "        time.sleep(0.4)\n",
    "        if page > 60:\n",
    "            break\n",
    "    return links[:required]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5929b1",
   "metadata": {},
   "source": [
    "Download images for a product (parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3cf873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. download_images_for_product\n",
    "def download_images_for_product(image_urls, pid, parallel=PARALLEL_DOWNLOADS, max_workers=MAX_WORKERS):\n",
    "    folder = os.path.join(IMAGES_DIR, pid)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image_paths = []\n",
    "    if not image_urls:\n",
    "        return []\n",
    "    if parallel and len(image_urls) > 1:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(download_image, url, folder): url for url in image_urls}\n",
    "            for fut in as_completed(futures):\n",
    "                res = fut.result()\n",
    "                if res:\n",
    "                    image_paths.append(res)\n",
    "    else:\n",
    "        for url in image_urls:\n",
    "            p = download_image(url, folder)\n",
    "            if p:\n",
    "                image_paths.append(p)\n",
    "            time.sleep(0.05)\n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baa0d1",
   "metadata": {},
   "source": [
    "Main orchestration: scrape, download, save products.csv/xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933d1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. main()\n",
    "def main(n_products=N_PRODUCTS, download_images=DOWNLOAD_IMAGES):\n",
    "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "    product_links = get_product_links_from_shop_pages(required=n_products)\n",
    "    print(f\"[main] Found {len(product_links)} product links. Sample: {product_links[:3]}\")\n",
    "    rows = []\n",
    "    for idx, url in enumerate(tqdm(product_links, desc=\"Scraping products\"), start=1):\n",
    "        rec = parse_product_page(url)\n",
    "        if not rec:\n",
    "            print(f\"[main] skip {url}\")\n",
    "            continue\n",
    "        pid = make_product_id(url)\n",
    "        rec[\"product_id\"] = pid\n",
    "        image_files = []\n",
    "        if download_images and rec.get(\"image_urls\"):\n",
    "            try:\n",
    "                image_files = download_images_for_product(rec[\"image_urls\"], pid)\n",
    "            except Exception as e:\n",
    "                print(f\"[main] image download error for {pid}: {e}\")\n",
    "        rec[\"image_files\"] = \"|\".join(image_files)\n",
    "        row = {\n",
    "            \"product_id\": rec[\"product_id\"],\n",
    "            \"product_line_name\": rec.get(\"brand\") or \"\",\n",
    "            \"brand_name\": rec.get(\"brand\") or \"\",\n",
    "            \"product_name\": rec.get(\"product_name\") or \"\",\n",
    "            \"product_description\": rec.get(\"description\") or \"\",\n",
    "            \"product_images\": \"|\".join(rec.get(\"image_urls\", [])),\n",
    "            \"barcode\": rec.get(\"sku\") or \"\",\n",
    "            \"price\": rec.get(\"price\") or \"\",\n",
    "            \"size_volume\": rec.get(\"size_volume\") or \"\",\n",
    "            \"ingredients\": rec.get(\"ingredients\") or \"\",\n",
    "            \"skin_concern\": rec.get(\"skin_concern\") or \"\",\n",
    "            \"image_files\": rec.get(\"image_files\") or \"\",\n",
    "            \"source_url\": rec.get(\"source_url\") or \"\"\n",
    "        }\n",
    "        rows.append(row)\n",
    "        if idx % SAVE_INTERVAL == 0:\n",
    "            save_partial(rows, csv_path=\"products_partial.csv\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[main] saved CSV => {OUTPUT_CSV}\")\n",
    "    try:\n",
    "        df.to_excel(OUTPUT_XLSX, index=False)\n",
    "        print(f\"[main] saved Excel => {OUTPUT_XLSX}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[main] could not save Excel ({e}). CSV saved instead.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7897c0",
   "metadata": {},
   "source": [
    "Run the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da76ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Found 12 product links. Sample: ['https://afroglamourcosmetics.com/product/lollis-beauty-make-up-professional-touch-smooth-primer-base-make-up-studio-finish-lp-155-2/', 'https://afroglamourcosmetics.com/product/lollis-beauty-make-up-real-look-mascara-with-keratin-lp-252-2/', 'https://afroglamourcosmetics.com/product/lollis-beauty-make-up-long-lasting-tattoo-waterproof-dipliner-lp-300-2/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:   8%|▊         | 1/12 [00:10<02:00, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg -> 403 Client Error: Forbidden for url: https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  17%|█▋        | 2/12 [00:19<01:37,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  25%|██▌       | 3/12 [00:27<01:18,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg -> 403 Client Error: Forbidden for url: https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  33%|███▎      | 4/12 [00:42<01:31, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  42%|████▏     | 5/12 [00:52<01:14, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg -> 403 Client Error: Forbidden for url: https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg\n",
      "[save_partial] saved products_partial.csv (5 rows)\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  50%|█████     | 6/12 [01:01<01:01, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  58%|█████▊    | 7/12 [01:10<00:48,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  67%|██████▋   | 8/12 [01:22<00:42, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg -> 403 Client Error: Forbidden for url: https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  75%|███████▌  | 9/12 [01:36<00:34, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  83%|████████▎ | 10/12 [01:47<00:22, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[save_partial] saved products_partial.csv (10 rows)\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/10/dr-rashel-black-soap-collagen-charcoal-soap-deep-cleansing-whitening-complex.jpg -> [Errno 2] No such file or directory: 'images\\\\palmolive-aroma-sensations-feel-good-oil-sensation-with-essential-oils-bright-shower-gel-500ml-2\\\\dr-rashel-black-soap-collagen-charcoal-soap-deep-cleansing-whitening-complex.jpg'\n",
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/07/Bio-Claire-Lightening-Body-Lotion-without-Hydroquinonen-210ml-100x100.jpg -> [Errno 2] No such file or directory: 'images\\\\palmolive-aroma-sensations-feel-good-oil-sensation-with-essential-oils-bright-shower-gel-500ml-2\\\\Bio-Claire-Lightening-Body-Lotion-without-Hydroquinonen-210ml-100x100.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products:  92%|█████████▏| 11/12 [01:58<00:11, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/07/Bio-Claire-Lightening-SetCream-Lotion-Serum-Soap-and-oil-600x593.jpeg -> [Errno 2] No such file or directory: 'images\\\\palmolive-aroma-sensations-feel-good-oil-sensation-with-essential-oils-bright-shower-gel-500ml-2\\\\Bio-Claire-Lightening-SetCream-Lotion-Serum-Soap-and-oil-600x593.jpeg'\n",
      "[download_image] failed data:image/svg+xml -> No connection adapters were found for 'data:image/svg+xml'\n",
      "[download_image] failed https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg -> 403 Client Error: Forbidden for url: https://afroglamourcosmetics.com/wp-content/uploads/2020/11/Clear-Essence-Platinum-Medicated-Fade-Creme-with-Sunscreen-4-oz..jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping products: 100%|██████████| 12/12 [02:09<00:00, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download_image] failed https://www.facebook.com/tr?id=306595973993558&ev=PageView&noscript=1 -> Downloaded file too small\n",
      "[main] saved CSV => products.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] saved Excel => products.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 9. Run scraper\n",
    "if __name__ == \"__main__\":\n",
    "    df_products = main(n_products=N_PRODUCTS, download_images=DOWNLOAD_IMAGES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce80e02",
   "metadata": {},
   "source": [
    "Verification (images exist and are referenced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81a77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in CSV: 12\n",
      "Rows with image_files: 12\n",
      "lollis-beauty-make-up-professional-touch-smooth-primer-base-make-up-studio-finish-lp-155-2 -> 49 images\n",
      "    images\\lollis-beauty-make-up-professional-touch-smooth-primer-base-make-up-studio-finish-lp-155-2\\glamour.png exists? True size: 4754\n",
      "    images\\lollis-beauty-make-up-professional-touch-smooth-primer-base-make-up-studio-finish-lp-155-2\\Perfume-Bundle-Afro-Glamour-Cosmetics.png exists? True size: 410679\n",
      "    images\\lollis-beauty-make-up-professional-touch-smooth-primer-base-make-up-studio-finish-lp-155-2\\wellbeingproducts.jpg exists? True size: 80680\n",
      "lollis-beauty-make-up-real-look-mascara-with-keratin-lp-252-2 -> 48 images\n",
      "    images\\lollis-beauty-make-up-real-look-mascara-with-keratin-lp-252-2\\glamour.png exists? True size: 4754\n",
      "    images\\lollis-beauty-make-up-real-look-mascara-with-keratin-lp-252-2\\facial-Care.jpg exists? True size: 14827\n",
      "    images\\lollis-beauty-make-up-real-look-mascara-with-keratin-lp-252-2\\hair-care.jpg exists? True size: 56062\n",
      "lollis-beauty-make-up-long-lasting-tattoo-waterproof-dipliner-lp-300-2 -> 49 images\n",
      "    images\\lollis-beauty-make-up-long-lasting-tattoo-waterproof-dipliner-lp-300-2\\glamour.png exists? True size: 4754\n",
      "    images\\lollis-beauty-make-up-long-lasting-tattoo-waterproof-dipliner-lp-300-2\\facial-Care.jpg exists? True size: 14827\n",
      "    images\\lollis-beauty-make-up-long-lasting-tattoo-waterproof-dipliner-lp-300-2\\hair-care.jpg exists? True size: 56062\n",
      "lollis-beauty-make-up-ultra-black-dipliner-lp-301-2 -> 48 images\n",
      "    images\\lollis-beauty-make-up-ultra-black-dipliner-lp-301-2\\glamour.png exists? True size: 4754\n",
      "    images\\lollis-beauty-make-up-ultra-black-dipliner-lp-301-2\\facial-Care.jpg exists? True size: 14827\n",
      "    images\\lollis-beauty-make-up-ultra-black-dipliner-lp-301-2\\makeups.jpg exists? True size: 59196\n",
      "st-ives-acne-control-apricot-scrub-283g-2 -> 47 images\n",
      "    images\\st-ives-acne-control-apricot-scrub-283g-2\\glamour.png exists? True size: 4754\n",
      "    images\\st-ives-acne-control-apricot-scrub-283g-2\\facial-Care.jpg exists? True size: 14827\n",
      "    images\\st-ives-acne-control-apricot-scrub-283g-2\\skin-care.jpg exists? True size: 63756\n",
      "Total image files on disk: 589\n"
     ]
    }
   ],
   "source": [
    "# 10. Verification\n",
    "import glob\n",
    "if not os.path.exists(OUTPUT_CSV):\n",
    "    print(\"CSV not found:\", OUTPUT_CSV)\n",
    "else:\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(\"Rows in CSV:\", len(df))\n",
    "    df['has_image_files'] = df['image_files'].notna() & (df['image_files'].str.len() > 0)\n",
    "    print(\"Rows with image_files:\", int(df['has_image_files'].sum()))\n",
    "    sample = df[df['has_image_files']].head(5)\n",
    "    for _, r in sample.iterrows():\n",
    "        pid = r['product_id']\n",
    "        files = r['image_files'].split(\"|\") if r['image_files'] else []\n",
    "        print(f\"{pid} -> {len(files)} images\")\n",
    "        for f in files[:3]:\n",
    "            exists = os.path.exists(f)\n",
    "            size = os.path.getsize(f) if exists else \"N/A\"\n",
    "            print(\"   \", f, \"exists?\", exists, \"size:\", size)\n",
    "    image_files_on_disk = list(glob.glob(os.path.join(IMAGES_DIR, \"**\", \"*.*\"), recursive=True))\n",
    "    print(\"Total image files on disk:\", len(image_files_on_disk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909fd62",
   "metadata": {},
   "source": [
    "Grouped ingredients: clean, explode, counts, mapping, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce9cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ingredient_summary.xlsx\n",
      "Saved: ingredient_counts.csv, ingredient_product_map.csv\n"
     ]
    }
   ],
   "source": [
    "# 11. Grouped ingredient table (runs after scraping)\n",
    "\n",
    "if not os.path.exists(OUTPUT_CSV):\n",
    "    raise FileNotFoundError(f\"{OUTPUT_CSV} not found. Run scraper first.\")\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "# helper to clean ingredient text\n",
    "def clean_ingredients_text(txt):\n",
    "    if pd.isna(txt) or str(txt).strip()==\"\":\n",
    "        return \"\"\n",
    "    s = str(txt)\n",
    "    s = re.sub(r'[\\r\\n]', ' ', s)\n",
    "    s = s.replace(';', ',').replace('|', ',').replace('/', ',')\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "    s = re.sub(r'\\s*,\\s*', ',', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r',+$', '', s)\n",
    "    return s\n",
    "\n",
    "df['ingredients_clean'] = df['ingredients'].fillna(\"\").apply(clean_ingredients_text)\n",
    "\n",
    "def split_ingredients(txt):\n",
    "    if not txt:\n",
    "        return []\n",
    "    parts = [p.strip() for p in txt.split(',') if p.strip()]\n",
    "    expanded = []\n",
    "    for p in parts:\n",
    "        for sub in re.split(r'\\sand\\s|\\s\\+\\s', p):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                expanded.append(sub)\n",
    "    return expanded\n",
    "\n",
    "df['ingredients_list'] = df['ingredients_clean'].apply(split_ingredients)\n",
    "df_exploded = df[['product_id','product_name','ingredients_list']].explode('ingredients_list').rename(columns={'ingredients_list':'ingredient'})\n",
    "df_exploded['ingredient'] = df_exploded['ingredient'].astype(str).str.strip()\n",
    "df_exploded = df_exploded[df_exploded['ingredient'] != \"\"].reset_index(drop=True)\n",
    "\n",
    "# simple normalization map — extend as needed\n",
    "norm_map = {\n",
    "    'aqua': 'water',\n",
    "    'water': 'water',\n",
    "    'glycerin': 'glycerin'\n",
    "    # add more mappings after inspection\n",
    "}\n",
    "\n",
    "def normalize_ingredient(ing):\n",
    "    ing0 = ing.strip().lower()\n",
    "    ing0 = re.sub(r'[^a-z0-9\\s\\-]', '', ing0)\n",
    "    ing0 = re.sub(r'\\s{2,}', ' ', ing0)\n",
    "    if ing0 in norm_map:\n",
    "        return norm_map[ing0]\n",
    "    return ing0\n",
    "\n",
    "df_exploded['ingredient_norm'] = df_exploded['ingredient'].apply(normalize_ingredient)\n",
    "\n",
    "# counts and mapping\n",
    "ingredient_counts = (\n",
    "    df_exploded.groupby('ingredient_norm')['product_id']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'product_id':'product_count'})\n",
    "    .sort_values('product_count', ascending=False)\n",
    ")\n",
    "\n",
    "ingredient_product_map = (\n",
    "    df_exploded.groupby('ingredient_norm')\n",
    "    .agg({\n",
    "        'product_id': lambda ids: \"|\".join(sorted(set(ids))),\n",
    "        'product_name': lambda names: \"|\".join(sorted(set(names)))\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'product_id':'product_ids','product_name':'product_names'})\n",
    ")\n",
    "\n",
    "ingredient_counts.to_csv(\"ingredient_counts.csv\", index=False)\n",
    "ingredient_product_map.to_csv(\"ingredient_product_map.csv\", index=False)\n",
    "\n",
    "# save both sheets to Excel if possible\n",
    "try:\n",
    "    with pd.ExcelWriter(\"ingredient_summary.xlsx\", engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        ingredient_counts.to_excel(writer, sheet_name=\"counts\", index=False)\n",
    "        ingredient_product_map.to_excel(writer, sheet_name=\"product_map\", index=False)\n",
    "    print(\"Saved ingredient_summary.xlsx\")\n",
    "except Exception as e:\n",
    "    print(\"Could not write Excel (engine missing?). CSVs saved instead.\", e)\n",
    "\n",
    "print(\"Saved: ingredient_counts.csv, ingredient_product_map.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be993a35",
   "metadata": {},
   "source": [
    "Zip images folder and final checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb324641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created images_archive.zip\n",
      "\n",
      "Deliverables produced:\n",
      "- products.csv\n",
      "- products.xlsx (if engine installed)\n",
      "- images/ (downloaded images by product_id)\n",
      "- images_archive.zip\n",
      "- ingredient_counts.csv\n",
      "- ingredient_product_map.csv\n",
      "- ingredient_summary.xlsx (if engine installed)\n",
      "- products_partial.csv (interim, optional)\n"
     ]
    }
   ],
   "source": [
    "# 12. Zip images and show final deliverables\n",
    "\n",
    "if os.path.exists(IMAGES_DIR) and any(os.scandir(IMAGES_DIR)):\n",
    "    shutil.make_archive(\"images_archive\", 'zip', IMAGES_DIR)\n",
    "    print(\"Created images_archive.zip\")\n",
    "else:\n",
    "    print(\"No images found to zip in\", IMAGES_DIR)\n",
    "\n",
    "print(\"\\nDeliverables produced:\")\n",
    "print(\"- products.csv\")\n",
    "print(\"- products.xlsx (if engine installed)\")\n",
    "print(\"- images/ (downloaded images by product_id)\")\n",
    "print(\"- images_archive.zip\")\n",
    "print(\"- ingredient_counts.csv\")\n",
    "print(\"- ingredient_product_map.csv\")\n",
    "print(\"- ingredient_summary.xlsx (if engine installed)\")\n",
    "print(\"- products_partial.csv (interim, optional)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
